{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (30,20)\n",
    "import time\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Admiralty: Japanese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HON10\n",
      "Shiki Etsu Japanese Restaurant\n",
      "HON10\n",
      "KOKOMI\n",
      "HON10\n",
      "KOKOMI\n",
      "hana-musubi\n",
      "Nadaman\n",
      "Nadaman\n",
      "Nakamura Tokichi\n",
      "HON10\n",
      "Hatsu Japanese Restaurant\n",
      "Shiki Etsu Japanese Restaurant\n",
      "Shiro\n",
      "KOKOMI\n",
      "Morikawa Bento\n",
      "KOKOMI\n",
      "KOKOMI\n",
      "Nadaman\n",
      "Morikawa Bento\n",
      "Nadaman\n",
      "Ippudo HK\n",
      "ROKA Japanese Robata Grill \n",
      "Ippudo HK\n",
      "Genki Sushi\n",
      "Ippudo HK\n",
      "Ippudo HK\n",
      "ROKA Japanese Robata Grill \n",
      "Hal's Japanese Restaurant\n",
      "ROKA Japanese Robata Grill \n",
      "Nadaman\n",
      "ROKA Japanese Robata Grill \n",
      "Shiki Japanese Restaurant\n",
      "Shiki Japanese Restaurant\n",
      "Genki Sushi\n",
      "Shiro\n",
      "ROKA Japanese Robata Grill \n",
      "ROKA Japanese Robata Grill \n",
      "Don-Curry Shop\n",
      "Nadaman\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.openrice.com/en/hongkong/restaurant/review/search.htm?smile=&daycategoryid=&pricetype=2&hasprice=1&district_id=1011&cuisine_id=2009&inputstrreview=\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "element = driver.find_element_by_xpath('//*[@id=\"togglefullreview\"]')\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "with open(\"Openrice_Admiralty_Japanese_1.csv\",\"a\") as f:\n",
    "    WriteFile = csv.writer(f)\n",
    "    WriteFile.writerow([\"District\",\"Cuisine\",\"Reviews\",\"Restaurant_Name\",\"Rating\",\"Cost\",\"Wait_Time\",\"Restuaruant_Link\"])\n",
    "\n",
    "district = \"Admiralty\"\n",
    "cuisine = \"Japanese\"\n",
    "\n",
    "nolu=2\n",
    "numberofloopspages = 1\n",
    "while numberofloopspages<4:\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "    for i in soup.find_all(\"div\", class_ = \"sr2_review_bk rel_pos\"):  \n",
    "        \n",
    "# review\n",
    "        try:\n",
    "            review_dirty = i.find(class_ = \"sr2_review_body div_br_word MB5\").get_text().strip().replace('\\n','').encode('ASCII', 'ignore')\n",
    "            review = review_dirty.decode(\"utf-8\")\n",
    "        except:\n",
    "            review = np.nan\n",
    "\n",
    "# restaurant name\n",
    "        try:\n",
    "            restaurant_name_dirty = i.find(\"a\", class_ = \"hiddenlink\").get_text().encode('ASCII', 'ignore')\n",
    "            restaurant_name = restaurant_name_dirty.decode(\"utf-8\")\n",
    "            print(restaurant_name)\n",
    "        except:\n",
    "            restaurant_name = np.nan\n",
    "\n",
    "# Rating\n",
    "        try:\n",
    "            rating_dirty = i.find(\"meta\", itemprop=\"ratingvalue\")\n",
    "            rating = re.findall(r\"\\d+\\.\\d+\", str(rating_dirty))[0]\n",
    "        except:\n",
    "            rating = np.nan\n",
    "\n",
    "# Cost  \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            cost = re.findall(r\"\\$(\\d+)\", str(details))[0]\n",
    "        except:\n",
    "            cost = np.nan\n",
    "\n",
    "# Wait time            \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            wait_time = re.findall(r'(\\d+?) min',str(details))[0]\n",
    "        except:\n",
    "            wait_time = np.nan\n",
    "            \n",
    "# restaurant link\n",
    "        try:\n",
    "            rest_link_dirty = i.find(class_ = \"hiddenlink\")\n",
    "            rest_link_clean = rest_link_dirty.get(\"href\")\n",
    "            rest_link = f\"www.openrice.com{rest_link_clean}\"\n",
    "        except:\n",
    "            rest_link = np.nan\n",
    "\n",
    "        \n",
    "        with open(\"Openrice_Admiralty_Japanese_1.csv\",\"a\") as f:\n",
    "            WriteFile = csv.writer(f)    \n",
    "            WriteFile.writerow([district, cuisine, review, restaurant_name, rating, cost, wait_time, rest_link]) \n",
    "            \n",
    "#     a = nolu + 1\n",
    "    a = nolu\n",
    "    numberofloopspagesxpath = '//*[@id=\"pagingContainer\"]/div/div['+str(a)+']/a'\n",
    "    \n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        nextpage = WebDriverWait(driver, 40).until(EC.visibility_of_element_located((By.XPATH, numberofloopspagesxpath)))\n",
    "#         print(nextpage)\n",
    "        nextpage.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        driver.get(driver.current_url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        numberofloopspages+=1 \n",
    "        nolu+=2 \n",
    "        if nolu>4:\n",
    "            nolu = 4\n",
    "\n",
    "    except TimeoutException:\n",
    "        print (\"Loading took too much time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Admiralty: Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim Sum Library\n",
      "Y Shanghai\n",
      "Dim Sum Library\n",
      "Dim Sum Library\n",
      "Tien Yi Chinese Restaurant\n",
      "Dim Sum Library\n",
      "Y Shanghai\n",
      "Metropol Restaurant\n",
      "Dim Sum Library\n",
      "Dim Sum Library\n",
      "Man Ho Chinese Restaurant\n",
      "Y Shanghai\n",
      "Good BBQ\n",
      "Dim Sum Library\n",
      "Dim Sum Library\n",
      "canteen\n",
      "East Ocean Seaview Restaurant\n",
      "LockCha Tea House\n",
      "Metropol Restaurant\n",
      "Metropol Restaurant\n",
      "LockCha Tea House\n",
      "canteen\n",
      "Summer Palace\n",
      "Crystal Jade La Mian Xiao Long Bao\n",
      "Zen\n",
      "Y Shanghai\n",
      "LockCha Tea House\n",
      "Summer Palace\n",
      "LockCha Tea House\n",
      "Shanghai Garden Restaurant\n",
      "Shanghai Garden Restaurant\n",
      "Man Ho Chinese Restaurant\n",
      "LockCha Tea House\n",
      "LockCha Tea House\n",
      "Fairwood\n",
      "LockCha Tea House\n",
      "Victoria City Restaurant\n",
      "Victoria City Restaurant\n",
      "Lippo Chiuchow Restaurant\n",
      "Victoria City Restaurant\n",
      "Shanghai Garden Restaurant\n",
      "Shanghai Garden Restaurant\n",
      "Lippo Chiuchow Restaurant\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.openrice.com/en/hongkong/restaurant/review/search.htm?smile=&daycategoryid=&pricetype=2&hasprice=1&district_id=1011&cuisine_id=1999&inputstrreview=\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "element = driver.find_element_by_xpath('//*[@id=\"togglefullreview\"]')\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "with open(\"Openrice_Admiralty_Chinese_1.csv\",\"a\") as f:\n",
    "    WriteFile = csv.writer(f)\n",
    "    WriteFile.writerow([\"District\",\"Cuisine\",\"Reviews\",\"Restaurant_Name\",\"Rating\",\"Cost\",\"Wait_Time\",\"Restuaruant_Link\"])\n",
    "\n",
    "district = \"Admiralty\"\n",
    "cuisine = \"Chinese\"\n",
    "\n",
    "nolu=2\n",
    "numberofloopspages = 1\n",
    "while numberofloopspages<4:\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "    for i in soup.find_all(\"div\", class_ = \"sr2_review_bk rel_pos\"):  \n",
    "        \n",
    "# review\n",
    "        try:\n",
    "            review_dirty = i.find(class_ = \"sr2_review_body div_br_word MB5\").get_text().strip().replace('\\n','').encode('ASCII', 'ignore')\n",
    "            review = review_dirty.decode(\"utf-8\")\n",
    "        except:\n",
    "            review = np.nan\n",
    "\n",
    "# restaurant name\n",
    "        try:\n",
    "            restaurant_name_dirty = i.find(\"a\", class_ = \"hiddenlink\").get_text().encode('ASCII', 'ignore')\n",
    "            restaurant_name = restaurant_name_dirty.decode(\"utf-8\")\n",
    "            print(restaurant_name)\n",
    "        except:\n",
    "            restaurant_name = np.nan\n",
    "\n",
    "# Rating\n",
    "        try:\n",
    "            rating_dirty = i.find(\"meta\", itemprop=\"ratingvalue\")\n",
    "            rating = re.findall(r\"\\d+\\.\\d+\", str(rating_dirty))[0]\n",
    "        except:\n",
    "            rating = np.nan\n",
    "\n",
    "# Cost  \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            cost = re.findall(r\"\\$(\\d+)\", str(details))[0]\n",
    "        except:\n",
    "            cost = np.nan\n",
    "\n",
    "# Wait time            \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            wait_time = re.findall(r'(\\d+?) min',str(details))[0]\n",
    "        except:\n",
    "            wait_time = np.nan\n",
    "            \n",
    "# restaurant link\n",
    "        try:\n",
    "            rest_link_dirty = i.find(class_ = \"hiddenlink\")\n",
    "            rest_link_clean = rest_link_dirty.get(\"href\")\n",
    "            rest_link = f\"www.openrice.com{rest_link_clean}\"\n",
    "        except:\n",
    "            rest_link = np.nan\n",
    "        \n",
    "        with open(\"Openrice_Admiralty_Chinese_1.csv\",\"a\") as f:\n",
    "            WriteFile = csv.writer(f)    \n",
    "            WriteFile.writerow([district, cuisine, review, restaurant_name, rating, cost, wait_time, rest_link]) \n",
    "\n",
    "    a = nolu\n",
    "    numberofloopspagesxpath = '//*[@id=\"pagingContainer\"]/div/div['+str(a)+']/a'\n",
    "    \n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        nextpage = WebDriverWait(driver, 40).until(EC.visibility_of_element_located((By.XPATH, numberofloopspagesxpath)))\n",
    "#         print(nextpage)\n",
    "        nextpage.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        driver.get(driver.current_url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        numberofloopspages+=1 \n",
    "        nolu+=2 \n",
    "        if nolu>4:\n",
    "            nolu = 4\n",
    "\n",
    "    except TimeoutException:\n",
    "        print (\"Loading took too much time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Admiralty: Thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thai Basil\n",
      "Thai Basil\n",
      "Apinara\n",
      "Apinara\n",
      "Apinara\n",
      "Apinara\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Loading took too much time\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Apinara\n",
      "Apinara\n",
      "Apinara\n",
      "Apinara\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Loading took too much time\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Apinara\n",
      "Apinara\n",
      "Apinara\n",
      "Apinara\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Thai Basil\n",
      "Thai Basil\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-49cba279021d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"window.scrollTo(0, document.body.scrollHeight);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mnextpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisibility_of_element_located\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumberofloopspagesxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;31m#         print(nextpage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mnextpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/support/wait.py\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'screen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mstacktrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stacktrace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "URL = \"https://www.openrice.com/en/hongkong/restaurant/review/search.htm?smile=&daycategoryid=&pricetype=2&hasprice=1&district_id=1011&cuisine_id=2004&inputstrreview=\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "element = driver.find_element_by_xpath('//*[@id=\"togglefullreview\"]')\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "with open(\"Openrice_Admiralty_Thai_1.csv\",\"a\") as f:\n",
    "    WriteFile = csv.writer(f)\n",
    "    WriteFile.writerow([\"District\",\"Cuisine\",\"Reviews\",\"Restaurant_Name\",\"Rating\",\"Cost\",\"Wait_Time\",\"Restuaruant_Link\"])\n",
    "\n",
    "district = \"Admiralty\"\n",
    "cuisine = \"Thai\"\n",
    "\n",
    "nolu=2\n",
    "numberofloopspages = 1\n",
    "while numberofloopspages==1:\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    for i in soup.find_all(\"div\", class_ = \"sr2_review_bk rel_pos\"):  \n",
    "        \n",
    "# review\n",
    "        try:\n",
    "            review_dirty = i.find(class_ = \"sr2_review_body div_br_word MB5\").get_text().strip().replace('\\n','').encode('ASCII', 'ignore')\n",
    "            review = review_dirty.decode(\"utf-8\")\n",
    "        except:\n",
    "            review = np.nan\n",
    "\n",
    "# restaurant name\n",
    "        try:\n",
    "            restaurant_name_dirty = i.find(\"a\", class_ = \"hiddenlink\").get_text().encode('ASCII', 'ignore')\n",
    "            restaurant_name = restaurant_name_dirty.decode(\"utf-8\")\n",
    "            print(restaurant_name)\n",
    "        except:\n",
    "            restaurant_name = np.nan\n",
    "\n",
    "# Rating\n",
    "        try:\n",
    "            rating_dirty = i.find(\"meta\", itemprop=\"ratingvalue\")\n",
    "            rating = re.findall(r\"\\d+\\.\\d+\", str(rating_dirty))[0]\n",
    "        except:\n",
    "            rating = np.nan\n",
    "\n",
    "# Cost  \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            cost = re.findall(r\"\\$(\\d+)\", str(details))[0]\n",
    "        except:\n",
    "            cost = np.nan\n",
    "\n",
    "# Wait time            \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            wait_time = re.findall(r'(\\d+?) min',str(details))[0]\n",
    "        except:\n",
    "            wait_time = np.nan\n",
    "            \n",
    "# restaurant link\n",
    "        try:\n",
    "            rest_link_dirty = i.find(class_ = \"hiddenlink\")\n",
    "            rest_link_clean = rest_link_dirty.get(\"href\")\n",
    "            rest_link = f\"www.openrice.com{rest_link_clean}\"\n",
    "        except:\n",
    "            rest_link = np.nan\n",
    "        \n",
    "        \n",
    "        with open(\"Openrice_Admiralty_Thai_1.csv\",\"a\") as f:\n",
    "            WriteFile = csv.writer(f)    \n",
    "            WriteFile.writerow([district, cuisine, review, restaurant_name, rating, cost, wait_time, rest_link]) \n",
    "            \n",
    "    a = nolu\n",
    "    numberofloopspagesxpath = '//*[@id=\"pagingContainer\"]/div/div['+str(a)+']/a'\n",
    "    \n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        nextpage = WebDriverWait(driver, 40).until(EC.visibility_of_element_located((By.XPATH, numberofloopspagesxpath)))\n",
    "#         print(nextpage)\n",
    "        nextpage.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        driver.get(driver.current_url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        numberofloopspages+=1 \n",
    "        nolu+=2 \n",
    "        if nolu>4:\n",
    "            nolu = 4\n",
    "\n",
    "    except TimeoutException:\n",
    "        print (\"Loading took too much time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Admiralty: Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nicholini's\n",
      "AMMO\n",
      "AMMO\n",
      "Operetta\n",
      "Operetta\n",
      "Operetta\n",
      "Operetta\n",
      "Nicholini's\n",
      "Nicholini's\n",
      "Operetta\n",
      "Operetta\n",
      "Operetta\n",
      "AMMO\n",
      "The Spaghetti House\n",
      "Nicholini's\n",
      "Nicholini's\n",
      "Great Salad Bar and Sandwich\n",
      "HABIT caff \n",
      "AMMO\n",
      "AMMO\n",
      "AMMO\n",
      "AMMO\n",
      "Grappa's Ristorante\n",
      "AMMO\n",
      "Domani\n",
      "Grappa's Ristorante\n",
      "The Spaghetti House\n",
      "Grappa's Ristorante\n",
      "Grappa's Ristorante\n",
      "Grappa's Ristorante\n",
      "Domani\n",
      "The Spaghetti House\n",
      "Domani\n",
      "Domani\n",
      "Nicholini's\n",
      "Domani\n",
      "Domani\n",
      "Domani\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.openrice.com/en/hongkong/restaurant/review/search.htm?smile=&daycategoryid=&pricetype=2&hasprice=1&district_id=1011&cuisine_id=3006&inputstrreview=\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "element = driver.find_element_by_xpath('//*[@id=\"togglefullreview\"]')\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "with open(\"Openrice_Admiralty_Italian_1.csv\",\"a\") as f:\n",
    "    WriteFile = csv.writer(f)\n",
    "    WriteFile.writerow([\"District\",\"Cuisine\",\"Reviews\",\"Restaurant_Name\",\"Rating\",\"Cost\",\"Wait_Time\",\"Restuaruant_Link\"])\n",
    "\n",
    "district = \"Admiralty\"\n",
    "cuisine = \"Italian\"\n",
    "\n",
    "nolu=2\n",
    "numberofloopspages = 1\n",
    "while numberofloopspages<4:\n",
    "    \n",
    "    time.sleep(5)\n",
    "    for i in soup.find_all(\"div\", class_ = \"sr2_review_bk rel_pos\"):  \n",
    "        \n",
    "# review\n",
    "        try:\n",
    "            review_dirty = i.find(class_ = \"sr2_review_body div_br_word MB5\").get_text().strip().replace('\\n','').encode('ASCII', 'ignore')\n",
    "            review = review_dirty.decode(\"utf-8\")\n",
    "        except:\n",
    "            review = np.nan\n",
    "\n",
    "# restaurant name\n",
    "        try:\n",
    "            restaurant_name_dirty = i.find(\"a\", class_ = \"hiddenlink\").get_text().encode('ASCII', 'ignore')\n",
    "            restaurant_name = restaurant_name_dirty.decode(\"utf-8\")\n",
    "            print(restaurant_name)\n",
    "        except:\n",
    "            restaurant_name = np.nan\n",
    "\n",
    "# Rating\n",
    "        try:\n",
    "            rating_dirty = i.find(\"meta\", itemprop=\"ratingvalue\")\n",
    "            rating = re.findall(r\"\\d+\\.\\d+\", str(rating_dirty))[0]\n",
    "        except:\n",
    "            rating = np.nan\n",
    "\n",
    "# Cost  \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            cost = re.findall(r\"\\$(\\d+)\", str(details))[0]\n",
    "        except:\n",
    "            cost = np.nan\n",
    "\n",
    "# Wait time            \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            wait_time = re.findall(r'(\\d+?) min',str(details))[0]\n",
    "        except:\n",
    "            wait_time = np.nan\n",
    "            \n",
    "# restaurant link\n",
    "        try:\n",
    "            rest_link_dirty = i.find(class_ = \"hiddenlink\")\n",
    "            rest_link_clean = rest_link_dirty.get(\"href\")\n",
    "            rest_link = f\"www.openrice.com{rest_link_clean}\"\n",
    "        except:\n",
    "            rest_link = np.nan\n",
    "        \n",
    "        with open(\"Openrice_Admiralty_Italian_1.csv\",\"a\") as f:\n",
    "            WriteFile = csv.writer(f)    \n",
    "            WriteFile.writerow([district, cuisine, review, restaurant_name, rating, cost, wait_time, rest_link]) \n",
    "            \n",
    "    a = nolu\n",
    "    numberofloopspagesxpath = '//*[@id=\"pagingContainer\"]/div/div['+str(a)+']/a'\n",
    "    \n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        nextpage = WebDriverWait(driver, 40).until(EC.visibility_of_element_located((By.XPATH, numberofloopspagesxpath)))\n",
    "        nextpage.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        driver.get(driver.current_url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#         element = driver.find_element_by_xpath('//*[@id=\"togglefullreview\"]')\n",
    "#         element.click()\n",
    "#         time.sleep(10)\n",
    "        \n",
    "        numberofloopspages+=1 \n",
    "        nolu+=2 \n",
    "        if nolu>4:\n",
    "            nolu = 4\n",
    "\n",
    "    except TimeoutException:\n",
    "        print (\"Loading took too much time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Admiralty: American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shake Shack\n",
      "Shake Shack\n",
      "Commissary\n",
      "Commissary\n",
      "Commissary\n",
      "Ruth's Chris Steak House\n",
      "Commissary\n",
      "Ruth's Chris Steak House\n",
      "Lawry's The Prime Rib\n",
      "McDonald's\n",
      "Commissary\n",
      "Lawry's The Prime Rib\n",
      "McDonald's Next\n",
      "Lawry's The Prime Rib\n",
      "Lawry's The Prime Rib\n",
      "McDonald's\n",
      "Dan Ryan's Chicago Grill\n",
      "Dan Ryan's Chicago Grill\n",
      "Ruth's Chris Steak House\n",
      "Ruth's Chris Steak House\n",
      "JW's California\n",
      "Burger King\n",
      "Dan Ryan's Chicago Grill\n",
      "Dan Ryan's Chicago Grill\n",
      "Dan Ryan's Chicago Grill\n",
      "Monster Burger \n",
      "Monster Burger \n",
      "Dan Ryan's Chicago Grill\n",
      "Monster Burger \n",
      "Dan Ryan's Chicago Grill\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.openrice.com/en/hongkong/restaurant/review/search.htm?smile=&daycategoryid=&pricetype=2&hasprice=1&district_id=1011&cuisine_id=4001&inputstrreview=\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "element = driver.find_element_by_xpath('//*[@id=\"togglefullreview\"]')\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "with open(\"Openrice_Admiralty_American_1.csv\",\"a\") as f:\n",
    "    WriteFile = csv.writer(f)\n",
    "    WriteFile.writerow([\"District\",\"Cuisine\",\"Reviews\",\"Restaurant_Name\",\"Rating\",\"Cost\",\"Wait_Time\",\"Restuaruant_Link\"])\n",
    "\n",
    "district = \"Admiralty\"\n",
    "cuisine = \"American\"\n",
    "\n",
    "nolu=2\n",
    "numberofloopspages = 1\n",
    "while numberofloopspages<3:\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "    for i in soup.find_all(\"div\", class_ = \"sr2_review_bk rel_pos\"):  \n",
    "        \n",
    "# review\n",
    "        try:\n",
    "            review_dirty = i.find(class_ = \"sr2_review_body div_br_word MB5\").get_text().strip().replace('\\n','').encode('ASCII', 'ignore')\n",
    "            review = review_dirty.decode(\"utf-8\")\n",
    "        except:\n",
    "            review = np.nan\n",
    "\n",
    "# restaurant name\n",
    "        try:\n",
    "            restaurant_name_dirty = i.find(\"a\", class_ = \"hiddenlink\").get_text().encode('ASCII', 'ignore')\n",
    "            restaurant_name = restaurant_name_dirty.decode(\"utf-8\")\n",
    "            print(restaurant_name)\n",
    "        except:\n",
    "            restaurant_name = np.nan\n",
    "\n",
    "# Rating\n",
    "        try:\n",
    "            rating_dirty = i.find(\"meta\", itemprop=\"ratingvalue\")\n",
    "            rating = re.findall(r\"\\d+\\.\\d+\", str(rating_dirty))[0]\n",
    "        except:\n",
    "            rating = np.nan\n",
    "\n",
    "# Cost  \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            cost = re.findall(r\"\\$(\\d+)\", str(details))[0]\n",
    "        except:\n",
    "            cost = np.nan\n",
    "\n",
    "# Wait time            \n",
    "        try:\n",
    "            details = i.find_all(class_= \"FR PT10 PB10\")\n",
    "            wait_time = re.findall(r'(\\d+?) min',str(details))[0]\n",
    "        except:\n",
    "            wait_time = np.nan\n",
    "            \n",
    "# restaurant link\n",
    "        try:\n",
    "            rest_link_dirty = i.find(class_ = \"hiddenlink\")\n",
    "            rest_link_clean = rest_link_dirty.get(\"href\")\n",
    "            rest_link = f\"www.openrice.com{rest_link_clean}\"\n",
    "        except:\n",
    "            rest_link = np.nan\n",
    "        \n",
    "        \n",
    "        with open(\"Openrice_Admiralty_American_1.csv\",\"a\") as f:\n",
    "            WriteFile = csv.writer(f)    \n",
    "            WriteFile.writerow([district, cuisine, review, restaurant_name, rating, cost, wait_time, rest_link]) \n",
    "            \n",
    "    a = nolu\n",
    "    numberofloopspagesxpath = '//*[@id=\"pagingContainer\"]/div/div['+str(a)+']/a'\n",
    "    \n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        nextpage = WebDriverWait(driver, 40).until(EC.visibility_of_element_located((By.XPATH, numberofloopspagesxpath)))\n",
    "#         print(nextpage)\n",
    "        nextpage.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        driver.get(driver.current_url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        numberofloopspages+=1 \n",
    "        nolu+=2 \n",
    "        if nolu>4:\n",
    "            nolu = 4\n",
    "\n",
    "    except TimeoutException:\n",
    "        print (\"Loading took too much time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
